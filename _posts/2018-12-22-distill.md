---
layout: distill
title: Audio-Visual Alignment and Learning
description: A comprehensive survey of audio visual learning techniques 
tags: distill formatting
giscus_comments: true
date: 2024-08-15
featured: true

authors:
  - name: Samarth Thopaiah
    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: Samsung Research, Carnegie Mellon

bibliography: 

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: High Level 
  - name: Reading List 
  - name: Broader Concepts in Audio Visual Learning 
  # - name: Code Blocks
  # - name: Interactive Plots
  # - name: Layouts
  # - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## High Level 

The general approach is obviously multi-modal, and typically consists of the following 
  - audio encoder 
  - video encoder 
  - feature aggregation step 
  - metric loss, generally unsupervised for pretraining, and a type of contrastive loss   

## Reading List 

1. SoundOfPixels 
2. DenseAV 
    Unsupervised audio-visual localization model, that is able to capture sounding locations, and localize semantic concepts from input speech, on the video frame. The audio encoder and video encoders are pretrained HuBERT and DINO models respectively, with minor modifications. 
    
    The feature aggregation is comprised of 

    Finally, the encoders are tuned with LoRA using InfoNCE loss.

    Although the large-scale undersupervised fine-tuning of the audio and video encoder, enables learning attention masks which localize the sources on the video, the model 

3. Mix and Localize 

    Similar to other audio visual localization approaches, where ResNets (without pre-training) based encoderes are used to generate audio and image based encodings. (k) audio embeddings are aligned to the image embeddings, using a cycle consistent random walk (and the associated loss).


    
4. EZ-VSL 
5. Light-ASD (Active Speaker Detection)
    
    Active Speaker Detection, is to detect whether the detected face is expressing speech (1) or not (0), for that temporal context. 

    A super lightweight model which has 3D CNN based audio and video encoders, further split into (2+1 D) convolutions to capture the spatial and temporal dimensions in the video encoding. A Bidirectional GRU is used at every timestep, on every face to determine whether the face expresses active speech or not. 
    
    Essentially the video encoder is capturing the variations in the facial features, while the audio encoder is capturing the variations in the MFCCs, and the GRU is aligning these features to determine whether speaking or not speaking. This is a supervised learning problem, using the AVA dataset. 


    A supervised loss to 

6. ImageBind
7. AVA-AVD

8. Annotation free Audio Visual Segmentation

    Proposes a 
9. 


## Broader Concepts in Audio-Visual learning (adopted from the mix and localize paper )


### Sound source separation. 

There has been a long history of methods for separating monaural sound mixtures. Early work addressed this problem with probabilistic models [15, 38] and recent work has tackled it via deep neural networks [22,51]. These often use a “mix-and-separate” [53] training procedure [26, 41]. Other work combines source separation with visual cues. Zhao et al. [53] proposed to separate different musical instruments by associating separated audio sources with pixels in the video, and later used optical flow [52] to provide motion cues. Gao et al. [18] jointly solve audio-visual speech separation with a multi-task learning framework by incorporating cross-modal face-voice attributes and lip motion. Tian et al. [42] jointly learn sound separation and sounding object visual grounding, using an approach they term cyclic co-learning. Chatterjee et al. [8] model visual signals into scene graphs and learn to separate sounds by co-segmenting subgraphs and associated audio. Majumder et al. [29] introduce the active audio-visual source separation task that an agent learns movement policies to improve sound separation qualities. Like these works, we jointly localize and separate sound. However, we do not generate separated audio: we obtain embeddings that represent the separated sound sources using contrastive learning.

### Audio-visual sound source localization. 

The co-occurrence of audio and visual cues in videos has been leveraged for sound source localization [19, 23, 35, 39, 43]. Researchers exploit audio-visual correspondence by matching audio and visual signals from the same video. Arandjelovic ́ and Zisserman [3, 4] measure the similarity between learned image region and audio representations and use multi-instance learning to localize sound sources. Owens and Efros [32] use class activation maps [54] to visualize the area contributing to solving audio-visual synchronization tasks. Chen et al. [9] mine hard negative image locations in cross-modal contrastive learning to obtain better sound localization results. Hu et al. [24] extend [4] and use clustering to generate pseudo-class labels, achieving class-aware sound source localization with mixed sounds. While we have a similar goal, our approach is entirely unsupervised, and does not require semi-supervised learning with labels, either at training or test time. Our work is motivated by them and aims to localize different sound sources in multi-source sound mixtures.

### Audio-visual self-supervision. 

Apart from sound source localization and separation, many recent works have pro- posed to use paired audio-visual data for representation learn- ing and other tasks as well. Owens et al. [33] learned visual representations for materials from impact sounds. Other work has learned features, scene structure, and geometric properties from sounds [11, 16, 34], or learns multisensory representations for both audio and vision [28, 32, 49]. Asano et al. [5] proposed self-supervised clustering and represen- tation learning approach for providing labels to multimodal data. Other work has learned active speaker detection [2, 14], up-mixing the mono audio [17, 36, 50], cross-modal distil- lation [6, 44]. We take inspiration from them and learn the representation of mixture sounds.


<!-- ## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

---

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

---

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

**Note:** `<d-code>` blocks do not look good in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:

{% highlight javascript %}
var x = 25;
function(x) {
return x \* x;
}
{% endhighlight %}

---

## Interactive Plots

You can add interative plots using plotly + iframes :framed_picture:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/demo.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:

{% highlight python %}
import pandas as pd
import plotly.express as px
df = pd.read_csv(
'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'
)
fig = px.density_mapbox(
df,
lat='Latitude',
lon='Longitude',
z='Magnitude',
radius=10,
center=dict(lat=0, lon=180),
zoom=0,
mapbox_style="stamen-terrain",
)
fig.show()
fig.write_html('assets/plotly/demo.html')
{% endhighlight %}

---

## Details boxes

Details boxes are collapsible boxes which hide additional information from the user. They can be added with the `details` liquid tag:

{% details Click here to know more %}
Additional details, where math $$ 2x - 1 $$ and `code` is rendered correctly.
{% enddetails %}

---

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

---

## Other Typography?

Emphasis, aka italics, with _asterisks_ (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or **underscores**.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
   ⋅⋅\* Unordered sub-list.
3. Actual numbers don't matter, just that it's a number
   ⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

- Unordered list can use asterisks

* Or minuses

- Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <http://www.example.com> and sometimes
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style:
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style:
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 2"

Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```

```python
s = "Python syntax highlighting"
print s
```

```
No language indicated, so no syntax highlighting.
But let's throw in a <b>tag</b>.
```

Colons can be used to align columns.

| Tables        |      Are      |  Cool |
| ------------- | :-----------: | ----: |
| col 3 is      | right-aligned | $1600 |
| col 2 is      |   centered    |   $12 |
| zebra stripes |   are neat    |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the
raw Markdown line up prettily. You can also use inline Markdown.

| Markdown | Less      | Pretty     |
| -------- | --------- | ---------- |
| _Still_  | `renders` | **nicely** |
| 1        | 2         | 3          |

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can _put_ **Markdown** into a blockquote.

Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a _separate paragraph_.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the _same paragraph_. -->
