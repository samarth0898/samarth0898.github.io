---
layout: distill
title: Binaural Audio Generation (constantly updated)
description: An exploration of neural networks for Binaural Audio Generation 
tags: distill formatting
giscus_comments: true
date: 2024-09-08
featured: false

authors:
  - name: Samarth Thopaiah
    affiliations:
      name: Samsung Research, Carnegie Mellon

bibliography: annontatedkan2024.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  # - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: High Level 
  - name: Sep - stereo 
  - name: Visually Informed Binaural Audio Generation without Binaural Audios 
  - name: Cyclic learning for Binaural Audio Generation 
  # - name: Code Blocks
  # - name: Interactive Plots
  # - name: Layouts
  # - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


*most of the excerpts are directly copied/referenced from the published papers*

## High Level

## Sep - stereo 


## Visually Informed Binaural Audio Generation without Binaural Audios 

This approach leverages spherical harmonic decomposition and head-related impulse response (HRIR) to identify the relationship between spatial locations and received binaural audios. Then in the visual modality, corresponding visual cues of the mono data are manually placed at sound source positions to form the pairs (training pairs).

- Generating synthetic data: Given 1 mono source, the method creates a *pseudo visual-stereo pair* by assigning the source direction in the spherical coordinates according to the manually created *visual-stereo pair*. Then the mono source is converted to binaural sources by using the Monoaural-Binaural mapping and spherical harmonic decomposition. 

- Spherical Harmonics: The key of this method relies on identifying the relationship between mono and stereo. This whole procedure is called Mono-Binaural-Mapping. Given a mono audio with an arbitrarily assigned source position ϑ = (ϑ, ϕ) the goal is to first convert it
to binaural channels with correct auditory sense of location. We empirically choose spherical harmonic decomposition for its expressive ability and its substantial connection with ambisonics. Finally, the decomposed coefficients are transformed to virtual array and rendered to audios with HRIR.

- Training 
    This method include two training targets, one for predicting the left and right stereophonic/binaural audio, and two, for the model to implicitly learn sound source separation. 

    - Stereo Training 
        The training procedure consists of a backbone U-Net audio network, and a ResNet18 visual network. The audios are all processed in the complex Time-Frequency domain in form of Short-Time Fourier Transformation (STFT). Mono is created from the left and right channels $s_{m}(t) = \hat{l}(t) + \hat{r}(t)$ (synthesized binaural audio) and the input to the audio network is the transformed mono spectrum $S_{m} = STFT(s_{m}(t))$. Audio network returns the complex mask M for final predictions. The training objective is the difference of the left and right spectrums SD = STFT(ˆl(t) − rˆ(t)), which can be written as:
        
        $$ L_{stereo} = \left\| S_{D} − M * S_{m}\right\|_{2}$$
    
    - Separation Training 



## Cyclic learning for Binaural Audio Generation 
*instead of suppressing the generated spectrograms like the previous method(sep-stereo), this method directly generates binaural audio , ingesting the mono audio, by using a diffusion model*

<figure>
<center>
    <img src="/assets/img/1.jpg" width="800" alt="CLBAG Teaser Figure">
    <figcaption>Teaser figure taken from the original <a
    href="">Cyclic learning for Binaural Audio Generation paper.</a> <d-cite key="li2024cyclic"></d-cite> </figcaption>
</center>
</figure>

This method analyzes the relationship between sounding object localization and binaural audio generation, and present a cyclic learning strategy achieving mutual benefits of them within a unified framework. They propose an object-aware upmixing model to generate binaural audio, meanwhile, design an OSA module and an SSM module to improve the localization performance of binaural audio. Experimental results demonstrate that the proposed approach can make sounding object localization and binaural audio generation promote each other and accomplish state-of-the-art performance.

- Visual Sounding Object Localization Model (OSA module)
  Given audio-visual content, which is naturally assumed to be aligned, positive and negative audio-visual pairs are developed for training. For the OSA module specifically, the authors argue that guidance information is typically embodied in the audiovisual correlation of the sounding object during binaural audio generation. A scene level and a frame level localization loss is defined, and learnt. *mathematical details are found in the paper*
    
- Semantic-spatial mining module
  To simultaneously mine the semantic and spatial information contained in bin- aural audio, we propose a semantic-spatial mining module.

- Object Aware Upmixing Model 
  It is built on the diffusion model and is mutually facilitated with the visual sounding object localization model through our cyclic learning strategy. We make two layers of 1D convolution to extract waveform features, the output features are concatenated with visual features and localized object features and then fused through a convolutional layer.
